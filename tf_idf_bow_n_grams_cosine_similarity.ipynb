{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TF - IDF (without inbuilt)**"
      ],
      "metadata": {
        "id": "GvIPoCmVSfwL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHVDlarMSXk4",
        "outputId": "920fcd81-5c99-4dfc-8d65-a86261bfd70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Term        TF       IDF    TF-IDF\n",
            "0       this  0.200000  0.000000  0.000000\n",
            "1         is  0.200000  0.000000  0.000000\n",
            "2        the  0.200000  0.000000  0.000000\n",
            "3      first  0.200000  0.693147  0.138629\n",
            "4   document  0.200000  0.287682  0.057536\n",
            "5       this  0.166667  0.000000  0.000000\n",
            "6   document  0.333333  0.287682  0.095894\n",
            "7         is  0.166667  0.000000  0.000000\n",
            "8        the  0.166667  0.000000  0.000000\n",
            "9     second  0.166667  1.386294  0.231049\n",
            "10  document  0.333333  0.287682  0.095894\n",
            "11       and  0.166667  1.386294  0.231049\n",
            "12      this  0.166667  0.000000  0.000000\n",
            "13        is  0.166667  0.000000  0.000000\n",
            "14       the  0.166667  0.000000  0.000000\n",
            "15     third  0.166667  1.386294  0.231049\n",
            "16       one  0.166667  1.386294  0.231049\n",
            "17        is  0.200000  0.000000  0.000000\n",
            "18      this  0.200000  0.000000  0.000000\n",
            "19       the  0.200000  0.000000  0.000000\n",
            "20     first  0.200000  0.693147  0.138629\n",
            "21  document  0.200000  0.287682  0.057536\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_document(doc):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    doc = doc.lower().translate(translator)\n",
        "    terms = doc.split()\n",
        "    return terms\n",
        "\n",
        "def calculate_tf(doc):\n",
        "    tf = {}\n",
        "    total_terms = len(doc)\n",
        "    for term in doc:\n",
        "        tf[term] = tf.get(term, 0) + 1 / total_terms\n",
        "    return tf\n",
        "\n",
        "def calculate_idf(documents):\n",
        "    N = len(documents)\n",
        "    idf = {}\n",
        "    term_document_count = {}\n",
        "    for doc in documents:\n",
        "        unique_terms = set(doc)\n",
        "        for term in unique_terms:\n",
        "            term_document_count[term] = term_document_count.get(term, 0) + 1\n",
        "    for term, count in term_document_count.items():\n",
        "        idf[term] = math.log(N / count)\n",
        "    return idf\n",
        "\n",
        "def calculate_tfidf(documents):\n",
        "    tfidf = []\n",
        "    idf = calculate_idf(documents)\n",
        "    for doc in documents:\n",
        "        tf = calculate_tf(doc)\n",
        "        doc_tfidf = {}\n",
        "        for term in doc:\n",
        "            if term in tf and term in idf:\n",
        "                doc_tfidf[term] = tf[term] * idf[term]\n",
        "        tfidf.append(doc_tfidf)\n",
        "    return tfidf\n",
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "preprocessed_docs = [preprocess_document(doc) for doc in documents]\n",
        "tfidf_scores = calculate_tfidf(preprocessed_docs)\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf_scores)\n",
        "df_tfidf.columns = sorted(set(term for doc in preprocessed_docs for term in doc))\n",
        "\n",
        "tf_scores = []\n",
        "idf_scores = calculate_idf(preprocessed_docs)\n",
        "for doc in preprocessed_docs:\n",
        "    tf_doc = calculate_tf(doc)\n",
        "    tf_scores.append(tf_doc)\n",
        "\n",
        "df_scores = pd.DataFrame()\n",
        "for i, doc in enumerate(preprocessed_docs):\n",
        "    doc_scores = pd.DataFrame({\n",
        "        'Term': doc,\n",
        "        'TF': [tf_scores[i].get(term, 0) for term in doc],\n",
        "        'IDF': [idf_scores.get(term, 0) for term in doc],\n",
        "        'TF-IDF': [tf_scores[i].get(term, 0) * idf_scores.get(term, 0) for term in doc]\n",
        "    })\n",
        "    df_scores = pd.concat([df_scores, doc_scores], ignore_index=True)\n",
        "\n",
        "print(df_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BAG OF WORDS**"
      ],
      "metadata": {
        "id": "nhGMnmsCSnpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I love natural language processing.\",\n",
        "    \"Natural language understanding is fascinating.\",\n",
        "    \"Processing and understanding are key NLP tasks.\",\n",
        "    \"this is my asssignment.\",\n",
        "]\n",
        "\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "vocabulary = set()\n",
        "for sentence in sentences:\n",
        "    words = tokenize(sentence)\n",
        "    vocabulary.update(words)\n",
        "\n",
        "bow_matrix = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = tokenize(sentence)\n",
        "    bow_vector = [0] * len(vocabulary)\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            index = list(vocabulary).index(word)\n",
        "            bow_vector[index] += 1\n",
        "    bow_matrix.append(bow_vector)\n",
        "\n",
        "for i, vector in enumerate(bow_matrix):\n",
        "    print(f\"Sentence {i+1} BoW Vector:\", vector)\n",
        "\n",
        "print(\"Vocabulary:\", list(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrsUhFb4SmxO",
        "outputId": "7f0bf00e-7a69-4a05-aef4-d0475590bcf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 BoW Vector: [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "Sentence 2 BoW Vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "Sentence 3 BoW Vector: [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
            "Sentence 4 BoW Vector: [1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "Vocabulary: ['this', 'my', 'and', 'processing.', 'tasks.', 'love', 'asssignment.', 'i', 'key', 'is', 'are', 'understanding', 'processing', 'fascinating.', 'nlp', 'natural', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-GRAMS**"
      ],
      "metadata": {
        "id": "5URltkjCSzNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    ngrams = []\n",
        "    if n > len(words):\n",
        "        return []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = \" \".join(words[i:i + n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "text = \"This is an example sentence for generating n-grams without built-in functions in Python\"\n",
        "n = 3\n",
        "result = generate_ngrams(text, n)\n",
        "for ngram in result:\n",
        "    print(ngram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubXPzIslSvXu",
        "outputId": "048ef39c-2677-4f2f-fdca-4de0be4a8076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an\n",
            "is an example\n",
            "an example sentence\n",
            "example sentence for\n",
            "sentence for generating\n",
            "for generating n-grams\n",
            "generating n-grams without\n",
            "n-grams without built-in\n",
            "without built-in functions\n",
            "built-in functions in\n",
            "functions in Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COSINE SIMALARITY**"
      ],
      "metadata": {
        "id": "nCZOsV0ZS3b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sentence1 = \"I enjoy playing football with my friends.\"\n",
        "sentence2 = \"He enjoy to play basketball.\"\n",
        "\n",
        "words1 = sentence1.split()\n",
        "words2 = sentence2.split()\n",
        "\n",
        "unique_words = set(words1 + words2)\n",
        "\n",
        "vector1 = [words1.count(word) for word in unique_words]\n",
        "vector2 = [words2.count(word) for word in unique_words]\n",
        "\n",
        "dot_product = sum(vector1[i] * vector2[i] for i in range(len(unique_words)))\n",
        "\n",
        "norm1 = np.linalg.norm(vector1)\n",
        "norm2 = np.linalg.norm(vector2)\n",
        "\n",
        "cosine_similarity = dot_product / (norm1 * norm2)\n",
        "\n",
        "print(\"Cosine Similarity:\", cosine_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOZ4-HvFS9Ad",
        "outputId": "7e9ec42e-cafe-458a-a872-4b4d680d5dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.16903085094570328\n"
          ]
        }
      ]
    }
  ]
}